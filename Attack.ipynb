{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attack.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "import tensorflow.keras as tk\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "#EPOCH = 100\n",
        "EPOCH = 5\n",
        "DATA_SIZE = 5000\n",
        "TRAINING_SIZE = 5000\n",
        "TEST_SIZE = 1000\n",
        "NUM_TARGET = 1\n",
        "#NUM_SHADOW = 100\n",
        "NUM_SHADOW = 3\n",
        "IN = 1\n",
        "OUT = 0\n",
        "VERBOSE = 1\n",
        "\n",
        "def sample_data(train_data,test_data,num_sets):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    new_x_train, new_y_train = [], []\n",
        "    new_x_test, new_y_test = [], []\n",
        "    for i in range(num_sets):\n",
        "        x_temp, y_temp = resample(x_train, y_train, n_samples=TRAINING_SIZE, random_state=0)\n",
        "        new_x_train.append(x_temp)\n",
        "        new_y_train.append(y_temp)\n",
        "        x_temp, y_temp = resample(x_test, y_test, n_samples=TEST_SIZE, random_state=0)\n",
        "        new_x_test.append(x_temp)\n",
        "        new_y_test.append(y_temp)\n",
        "    return (new_x_train, new_y_train), (new_x_test, new_y_test)\n",
        "\n",
        "def build_fcnn_model():\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28,28)))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_trained_keras_models(keras_model, train_data, test_data, num_models):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        models.append(keras.models.clone_model(keras_model))\n",
        "        models[i].compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        models[i].fit(x_train[i], y_train[i], batch_size=32, epochs=EPOCH, verbose=VERBOSE, shuffle=True)\n",
        "        score = models[i].evaluate(x_test[i], y_test[i], verbose=VERBOSE)\n",
        "        print('\\n', 'Model ', i, ' test accuracy:', score[1])\n",
        "    return models\n",
        "\n",
        "def get_attack_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_classes = len(y_train[0][0])\n",
        "    x_data, y_data = [[] for i in range(num_classes)], [[] for i in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(IN)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(OUT)\n",
        "    return x_data, y_data\n",
        "\n",
        "def get_trained_svm_models(train_data, test_data, num_models):\n",
        "    from sklearn import svm\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        print('Training svm model : ', i)\n",
        "        models.append(svm.SVC(gamma='scale',kernel='linear',verbose=VERBOSE))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score = models[i].score(x_test[i],y_test[i])\n",
        "        print('SVM model ', i, 'score : ',score)\n",
        "    return models\n",
        "\n",
        "def main():\n",
        "    print('Hello World!')\n",
        "    # load the pre-shuffled train and test data\n",
        "    mnist = tk.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    # rescale [0,255] --> [0,1]\n",
        "    x_train = x_train.astype('float32')/255\n",
        "    x_test = x_test.astype('float32')/255\n",
        "    # one-hot encoding for the labels\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    # split the data for each model\n",
        "    target_train = (x_train[:TRAINING_SIZE*NUM_TARGET],y_train[:TRAINING_SIZE*NUM_TARGET])\n",
        "    target_test = (x_test[:TEST_SIZE*NUM_TARGET],y_test[:TEST_SIZE*NUM_TARGET])\n",
        "    target_train_data, target_test_data = sample_data(target_train, target_test, NUM_TARGET)\n",
        "\n",
        "    shadow_train = (x_train[TRAINING_SIZE*NUM_TARGET:],y_train[TRAINING_SIZE*NUM_TARGET:])\n",
        "    shadow_test = (x_test[TEST_SIZE*NUM_TARGET:],y_test[TEST_SIZE*NUM_TARGET:])\n",
        "    shadow_train_data, shadow_test_data = sample_data(shadow_train, shadow_test, NUM_SHADOW)\n",
        "\n",
        "    cnn_model = build_fcnn_model()\n",
        "    # compile the target model\n",
        "    target_models = get_trained_keras_models(cnn_model, target_train_data, target_test_data, NUM_TARGET)\n",
        "    # compile the shadow models\n",
        "    shadow_models = get_trained_keras_models(cnn_model, shadow_train_data, shadow_test_data, NUM_SHADOW)\n",
        "\n",
        "    # get train data for the attack model\n",
        "    attack_train = get_attack_dataset(shadow_models, shadow_train_data, shadow_test_data, NUM_SHADOW, TEST_SIZE)\n",
        "    # get test data for the attack model\n",
        "    attack_test = get_attack_dataset(target_models, target_train_data, target_test_data, NUM_TARGET, TEST_SIZE)\n",
        "\n",
        "    # training the attack model\n",
        "    attack_model = get_trained_svm_models(attack_train, attack_test, num_classes)\n",
        "\n",
        "    # TODO generate the report\n",
        "    # scores = get_score_svm_models(attack_model, attack_test)\n",
        "    # _LOG_PRINT(scores)\n",
        "    # print(scores)\n",
        "\n",
        "    # return scores\n",
        "\n",
        "\n",
        "if __name__== '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCLbzwUYiKBN",
        "outputId": "0f8b6936-ba86-40f7-842a-52bfe18f8118"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 3s 3ms/step - loss: 0.4339 - accuracy: 0.8714\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9614\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9786\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 0.9900\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.9934\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.9140\n",
            "\n",
            " Model  0  test accuracy: 0.9139999747276306\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.5082 - accuracy: 0.8372\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9348\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9636\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9784\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9874\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.9330\n",
            "\n",
            " Model  0  test accuracy: 0.9330000281333923\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.8382\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9382\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1138 - accuracy: 0.9634\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9782\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.9868\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2636 - accuracy: 0.9360\n",
            "\n",
            " Model  1  test accuracy: 0.9359999895095825\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.5260 - accuracy: 0.8300\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 0.2030 - accuracy: 0.9360\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9648\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9780\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9856\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9430\n",
            "\n",
            " Model  2  test accuracy: 0.9430000185966492\n",
            "Training svm model :  0\n",
            "[LibSVM]SVM model  0 score :  0.5254237288135594\n",
            "Training svm model :  1\n",
            "[LibSVM]SVM model  1 score :  0.5833333333333334\n",
            "Training svm model :  2\n",
            "[LibSVM]SVM model  2 score :  0.5104602510460251\n",
            "Training svm model :  3\n",
            "[LibSVM]SVM model  3 score :  0.45664739884393063\n",
            "Training svm model :  4\n",
            "[LibSVM]SVM model  4 score :  0.5485436893203883\n",
            "Training svm model :  5\n",
            "[LibSVM]SVM model  5 score :  0.6134969325153374\n",
            "Training svm model :  6\n",
            "[LibSVM]SVM model  6 score :  0.5674418604651162\n",
            "Training svm model :  7\n",
            "[LibSVM]SVM model  7 score :  0.5067873303167421\n",
            "Training svm model :  8\n",
            "[LibSVM]SVM model  8 score :  0.6407185628742516\n",
            "Training svm model :  9\n",
            "[LibSVM]SVM model  9 score :  0.47533632286995514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.utils import resample, shuffle\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.utils import np_utils\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "IN = 1\n",
        "OUT = 0\n",
        "VERBOSE = 0\n",
        "_LOG_PRINT = lambda *a: None\n",
        "\n",
        "def load_cifar10(num_class=10):\n",
        "    import keras\n",
        "    # load the pre-shuffled train and test data\n",
        "    # mnist = tk.datasets.cifar10\n",
        "    # (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "    # rescale [0,255] --> [0,1]\n",
        "    x_train = x_train.astype('float32')/255\n",
        "    x_test = x_test.astype('float32')/255\n",
        "    # one-hot encoding for the labels\n",
        "    class_size = len(np.unique(y_train))\n",
        "    y_train = keras.utils.np_utils.to_categorical(y_train, class_size)\n",
        "    y_test = keras.utils.np_utils.to_categorical(y_test, class_size)\n",
        "    # reduce the class size\n",
        "    y_train = y_train[:,:num_class]\n",
        "    y_test = y_test[:,:num_class]\n",
        "\n",
        "    shuffle(x_train, y_train, random_state=0)\n",
        "    shuffle(x_test, y_test, random_state=0)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def sample_data(train_data, test_data, training_size, test_size, num_sets):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    new_x_train, new_y_train = [], []\n",
        "    new_x_test, new_y_test = [], []\n",
        "    for _ in range(num_sets):\n",
        "        x_temp, y_temp = resample(x_train, y_train, n_samples=training_size, random_state=0)\n",
        "        new_x_train.append(x_temp)\n",
        "        new_y_train.append(y_temp)\n",
        "        x_temp, y_temp = resample(x_test, y_test, n_samples=test_size, random_state=0)\n",
        "        new_x_test.append(x_temp)\n",
        "        new_y_test.append(y_temp)\n",
        "    return (new_x_train, new_y_train), (new_x_test, new_y_test)\n",
        "\n",
        "def build_cnn_model(num_class=10):\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='tanh', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='tanh'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='tanh'))\n",
        "    model.add(Dense(64, activation='tanh'))\n",
        "    if num_class==1:\n",
        "        model.add(Dense(num_class, activation='sigmoid'))    \n",
        "    else:\n",
        "        model.add(Dense(num_class, activation='softmax')) \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_keras_models(keras_model, num_class, num_models):\n",
        "    # import keras\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        models.append(keras.models.clone_model(keras_model))\n",
        "        rms = keras.optimizers.RMSprop(lr=LEARNING_RATE, decay=1e-7)\n",
        "        sgd = keras.optimizers.SGD(lr=LEARNING_RATE, decay=1e-7)\n",
        "        if num_class == 1 :\n",
        "            models[i].compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "        else:\n",
        "            models[i].compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "    return models\n",
        "\n",
        "def train_keras_models(models, train_data, test_data, epochs):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    for i in range(len(models)):\n",
        "        history = models[i].fit(x_train[i], y_train[i], epochs=epochs, verbose=VERBOSE, shuffle=True, batch_size=32)\n",
        "        print(history)\n",
        "        train_accs.append(history.history['accuracy'][-1])\n",
        "        score = models[i].evaluate(x_test[i], y_test[i], verbose=VERBOSE)\n",
        "        test_accs.append(score)\n",
        "        _LOG_PRINT('\\n', 'Model ', i, ' test accuracy:', score[1])\n",
        "    return (train_accs, test_accs)\n",
        "\n",
        "def get_attack_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_class = len(y_train[0][0])\n",
        "    x_data, y_data = [[] for _ in range(num_class)], [[] for _ in range(num_class)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(IN)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(OUT)\n",
        "    return x_data, y_data\n",
        "\n",
        "def get_trained_svm_models(train_data, test_data):\n",
        "    from sklearn import svm\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_models = len(y_train)\n",
        "    models = []\n",
        "    score_sum = 0\n",
        "    for i in range(num_models):\n",
        "        _LOG_PRINT('Training svm model : ', i)\n",
        "        models.append(svm.SVC(gamma='scale',kernel='linear',verbose=VERBOSE))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score = models[i].score(x_test[i],y_test[i])\n",
        "        score_sum = score_sum + score\n",
        "        _LOG_PRINT('SVM model ', i, 'score : ',score)\n",
        "    _LOG_PRINT('Total attack score : ', score_sum/num_models)\n",
        "    return models\n",
        "\n",
        "def get_score_svm_models(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "    return (acc_scores, pre_scores, rec_scores)\n",
        "\n",
        "def main(num_target=1, num_shadow=10, training_size=5000, test_size=1000, epochs=10, num_class=10): \n",
        "    def split_pair(x_data, y_data, split_point):\n",
        "        assert len(x_data) == len(y_data)\n",
        "        assert len(x_data) > split_point\n",
        "        sp = split_point\n",
        "        return ((x_data[:sp],y_data[:sp]), (x_data[sp:],y_data[sp:]))\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10(num_class)\n",
        "\n",
        "    # split the data for each model\n",
        "    split_point = training_size*num_target\n",
        "    train_data = split_pair(x_train, y_train, split_point)\n",
        "    split_point = test_size*num_target\n",
        "    test_data = split_pair(x_test, y_test, split_point)\n",
        "\n",
        "    target_train, target_test = sample_data(train_data[0], test_data[0], training_size, test_size, num_target)\n",
        "    shadow_train, shadow_test = sample_data(train_data[1], test_data[1], training_size, test_size, num_shadow)\n",
        "\n",
        "    cnn_model = build_cnn_model(num_class)\n",
        "    # compile the target model\n",
        "    target_models = get_keras_models(cnn_model, num_class, num_target)\n",
        "    train_keras_models(target_models, target_train, target_test, epochs)\n",
        "    # compile the shadow models\n",
        "    shadow_models = get_keras_models(cnn_model, num_class, num_shadow)\n",
        "    train_keras_models(shadow_models, shadow_train, shadow_test, epochs)\n",
        "\n",
        "    # get train data for the attack model\n",
        "    attack_train = get_attack_dataset(shadow_models, shadow_train, shadow_test, num_shadow, test_size)\n",
        "    # get test data for the attack model\n",
        "    attack_test = get_attack_dataset(target_models, target_train, target_test, num_target, test_size)\n",
        "\n",
        "    # training the attack model\n",
        "    attack_model = get_trained_svm_models(attack_train, attack_test)\n",
        "\n",
        "    # TODO generate the report\n",
        "    scores = get_score_svm_models(attack_model, attack_test)\n",
        "    _LOG_PRINT(scores)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Experiments\n",
        "def size_class_exp(num_shadow=100, epochs=100, result_file='result.csv'):\n",
        "    result = [['training_size']+[i for i in range(1,11)]]\n",
        "    for training_size in [2500, 5000, 10000, 15000]:\n",
        "        accuracy, precision, recall = [training_size], [training_size], [training_size]\n",
        "        for num_class in range(1,11):\n",
        "            _LOG_PRINT('ts : ', training_size, 'cl : ', num_class)\n",
        "            scores = main(1,num_shadow,training_size,2000,epochs,num_class)\n",
        "            accuracy.append(sum(scores[0])/len(scores[0]))\n",
        "            precision.append(sum(scores[1])/len(scores[1]))\n",
        "            recall.append(sum(scores[2])/len(scores[2]))\n",
        "        result.append(accuracy)\n",
        "        result.append(precision)\n",
        "        result.append(recall)\n",
        "    return result\n",
        "\n",
        "def shadow_num_exp(training_size=5000, test_size=1000, epochs=10, num_class=10, result_file='result.csv'):\n",
        "    shadow_sizes = [1,10,50,100]\n",
        "    result = [shadow_sizes]\n",
        "    accuracy, precision, recall = [], [], []\n",
        "    for num_shadow in shadow_sizes:\n",
        "        scores = main(1,num_shadow,training_size,test_size,epochs,num_class)\n",
        "        accuracy.append(sum(scores[0])/len(scores[0]))\n",
        "        precision.append(sum(scores[1])/len(scores[1]))\n",
        "        recall.append(sum(scores[2])/len(scores[2]))\n",
        "    result.append(accuracy)\n",
        "    result.append(precision)\n",
        "    result.append(recall)\n",
        "    return result\n",
        "\n",
        "def overfitting_exp(num_shadow=10,training_size=5000, test_size=1000, num_class=10, result_file='result.csv'):\n",
        "    epochs_sizes = [10, 50, 100, 200, 500]\n",
        "    result = [epochs_sizes]\n",
        "    accuracy, precision, recall = [], [], []\n",
        "    for epochs in epochs_sizes:\n",
        "        scores = main(1,num_shadow,training_size,test_size,epochs,num_class)\n",
        "        accuracy.append(sum(scores[0])/len(scores[0]))\n",
        "        precision.append(sum(scores[1])/len(scores[1]))\n",
        "        recall.append(sum(scores[2])/len(scores[2]))\n",
        "    result.append(accuracy)\n",
        "    result.append(precision)\n",
        "    result.append(recall)\n",
        "    return result\n",
        "\n",
        "result = main()\n",
        "    \n",
        "print(result)\n",
        "\n",
        "# if __name__== '__main__':\n",
        "#     parser = argparse.ArgumentParser(description='Member inference attack experiment for CIFAR10')\n",
        "#     parser.add_argument('-t', '--num_target', type=int, default=1)\n",
        "#     parser.add_argument('-s', '--num_shadow', type=int, default=10)\n",
        "#     parser.add_argument('--training_size', type=int, default=5000)\n",
        "#     parser.add_argument('--test_size', type=int, default=1000)\n",
        "#     parser.add_argument('-e', '--epochs', type=int, default=10)\n",
        "#     parser.add_argument('-c', '--num_class', type=int, default=10)\n",
        "#     parser.add_argument('-v', '--verbose', action='count', help='verbose mode')\n",
        "#     parser.add_argument('-r', '--result_file', default='result.csv', type=str, help='file name for the result')\n",
        "#     parser.add_argument('--size_class_exp', action='count', help='experiment with training size and class size')\n",
        "#     parser.add_argument('--shadow_exp', action='count', help='experiment with shadow size')\n",
        "#     parser.add_argument('--overfit_exp', action='count', help='experiment with overfitting')\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     if args.verbose:\n",
        "#         _LOG_PRINT = print\n",
        "#         VERBOSE = 1\n",
        "\n",
        "#     if args.size_class_exp:\n",
        "#         result = size_class_exp(args.num_shadow, args.epochs)\n",
        "#     elif args.shadow_exp:\n",
        "#         result = shadow_num_exp(args.training_size, args.test_size, args.epochs, args.num_class)\n",
        "#     elif args.overfit_exp:\n",
        "#         result = overfitting_exp(args.num_shadow,args.training_size, args.test_size, args.num_class)\n",
        "#     else:\n",
        "#         result = main(args.num_target, args.num_shadow, args.training_size, args.test_size, args.epochs, args.num_class)\n",
        "    \n",
        "#     with open(args.result_file, 'w', newline='') as csvfile:\n",
        "#         writer = csv.writer(csvfile)\n",
        "#         for r in result:\n",
        "#             writer.writerow(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSsrlw6TiYKr",
        "outputId": "72aa3732-09d7-4840-b31d-886e48f0d3aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 16)        208       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 16, 16, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 32)        2080      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 8, 8, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               262272    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 273,466\n",
            "Trainable params: 273,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.callbacks.History object at 0x7fd9f64dca10>\n",
            "<keras.callbacks.History object at 0x7fd9fc360350>\n",
            "<keras.callbacks.History object at 0x7fd9fc47a890>\n",
            "<keras.callbacks.History object at 0x7fd9fc9fe4d0>\n",
            "<keras.callbacks.History object at 0x7fd9f61be7d0>\n",
            "<keras.callbacks.History object at 0x7fd9f6287790>\n",
            "<keras.callbacks.History object at 0x7fd9fc255e90>\n",
            "<keras.callbacks.History object at 0x7fd9fc3cd3d0>\n",
            "<keras.callbacks.History object at 0x7fd9f782ced0>\n",
            "<keras.callbacks.History object at 0x7fd9ca5e9dd0>\n",
            "<keras.callbacks.History object at 0x7fd9ca3f48d0>\n",
            "([0.6702127659574468, 0.7168674698795181, 0.6046511627906976, 0.841025641025641, 0.7676767676767676, 0.8333333333333334, 0.6949152542372882, 0.6614583333333334, 0.6606334841628959, 0.8325581395348837], [0.5821917808219178, 0.626984126984127, 0.559322033898305, 0.8181818181818182, 0.7014925373134329, 0.8288288288288288, 0.6134969325153374, 0.5793103448275863, 0.6065573770491803, 0.7762237762237763], [0.9883720930232558, 1.0, 0.9339622641509434, 0.8910891089108911, 0.94, 0.9019607843137255, 0.9174311926605505, 0.9545454545454546, 0.9736842105263158, 0.9652173913043478])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BowmocusbQV",
        "outputId": "6656d950-9ac1-4442-e97f-f25b0c5e9d9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMQz3avhrJIG",
        "outputId": "d8e34620-7023-4f38-be9f-17ef374fb169"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AdrienBenamira/membership_inference_attack"
      ],
      "metadata": {
        "id": "a3geUlolnebV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AdrienBenamira/membership_inference_attack.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-Dd6IPvSiB",
        "outputId": "f8fe5a23-0e74-4ed5-fd5e-1914cc06e704"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'membership_inference_attack'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"./membership_inference_attack\")"
      ],
      "metadata": {
        "id": "wg_RfG9TwIfh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p8Ozwf3wi0b",
        "outputId": "8b28d65a-7d86-463b-db20-582c07b6f1a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config\t\t\texperience_cifar10.py  model.py     results\n",
            "dataloaders.py\t\texperience_mnist.py    __pycache__  trainer.py\n",
            "experience_cifar100.py\tmain.py\t\t       README.md    utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQNdWtphwqfC",
        "outputId": "87887f2b-a7be-491c-85c2-e6e39576bfe1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START STATS ON NUMBER SHADOW  WITH MNIST :  [100]\n",
            "START MNIST\n",
            "START TRAINING TARGET MODEL\n",
            "TAILLE dataset {'train': 2500, 'val': 1000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Training complete in 0m 8s\n",
            "DONE TRAIN\n",
            "START TRAINING SHADOW MODEL\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 7s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "TAILLE dataset {'train': 57500, 'val': 9000}\n",
            "DATASET SIZE {'train': 2500, 'val': 1000}\n",
            "Training complete in 0m 6s\n",
            "DONE TRAIN\n",
            "START GETTING DATASET ATTACK MODEL\n",
            "Taille dataset train 350000\n",
            "Taille dataset test 3500\n",
            "START FITTING ATTACK MODEL\n",
            "END MNIST\n",
            "Accuracy\n",
            "[0.748]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8PPMhna6w7Zr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}